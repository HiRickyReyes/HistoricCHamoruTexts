{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attached you will find my code for a Digital Humanities project that:\n",
    "\n",
    "- Collected and digitized historic texts on Guam and CHamoru culture\n",
    "- Created TEI files for analysis of the corpus\n",
    "- Seeked to use TF-IDF, co-occurrence, sentiment analysis, and topic modeling to explore differences between how insiders (those from Guam and who identify as CHamrou) and outsiders (those who write ABOUT Guam but are not from or of the culture) talk about the island.\n",
    "\n",
    "This is a starting point and uses distant reading to point towards potential future close-reading and digitization efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs \n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1: Prepare texts for analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/ricky/digital_texts/corpus/files/0_tei_files/finalized_tei/marianas_mosiac.xml\n",
      "Processing: /Users/ricky/digital_texts/corpus/files/0_tei_files/finalized_tei/guam_two_invasions_and_three_military_occupations.xml\n",
      "Processing: /Users/ricky/digital_texts/corpus/files/0_tei_files/finalized_tei/legacy_of_a_political_union.xml\n",
      "Processing: /Users/ricky/digital_texts/corpus/files/0_tei_files/finalized_tei/eng_chamoru_legends.xml\n",
      "Processing: /Users/ricky/digital_texts/corpus/files/0_tei_files/finalized_tei/destinys_landfall.xml\n",
      "Processing: /Users/ricky/digital_texts/corpus/files/0_tei_files/finalized_tei/history_of_the_chamorro_people.xml\n",
      "Processing done.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def a function for pre-processing\n",
    "def pre_process(text):\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = [token.lemma_ for token in doc if token.pos_ in ['NOUN', 'ADJ']]\n",
    "    return \" \".join(processed_tokens)   # joined processed tokens consisting of pre-processed nouns and adjectives\n",
    "\n",
    "# use os.scandir to apply function to files in path\n",
    "filepath = \"/Users/ricky/digital_texts/corpus/files/0_tei_files/finalized_tei\"\n",
    "metadata_list = []\n",
    "text_list = []\n",
    "raw_text_list = []\n",
    "\n",
    "for entry in os.scandir(filepath):\n",
    "    # control for hidden files (realized this was providing an issue for processing)\n",
    "    if entry.name.startswith('.'):\n",
    "        continue\n",
    "    print(f'Processing: {entry.path}')  # track processing time\n",
    "    try:\n",
    "        with open(entry.path, encoding='utf8') as file:\n",
    "            xml_content = file.read()\n",
    "            # parsing file w/ BeautifulSoup to extract metadata\n",
    "            soup = bs(xml_content, \"xml\")\n",
    "            author = soup.author.text.strip() if soup.author else \"Unknown\"\n",
    "            title = soup.title.text.strip() if soup.title else \"Untitled\"\n",
    "            pub_date = soup.date.text.strip() if soup.date else \"Unknown\"\n",
    "            in_out = soup.affiliation.text.strip() if soup.affiliation else \"Unknown\" \n",
    "            # store metadata\n",
    "            metadata = {\"author\": author, \"title\": title, \"pub_date\": pub_date, \"insider/outsider\": in_out,}\n",
    "            metadata_list.append(metadata)\n",
    "            \n",
    "            # pre processing continued with text extraction\n",
    "            text = soup.body.text if soup.body else \"\"\n",
    "            raw_text_list.append(text)  # append raw text to a list for future processing\n",
    "            processed_text = pre_process(text)\n",
    "            text_list.append(processed_text)    # append processed text list for future processing\n",
    "    except UnicodeDecodeError:  # more control measures for hidden files\n",
    "        print(f'UnicodeDecodeError in: {entry.path}')\n",
    "        continue\n",
    "print('Processing done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2: Analysis of text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES ON VECTORIZING:\n",
    "- Vectorizing is the process of converting raw text data into numerical representations (vectors) that machine learning algorithms can understand and process. Since algorithms work with numbers, not words, this step is essential for tasks like text classification, clustering, or analysis.\n",
    "- CountVectorizer (Bag-Of-Words) counts how many times each word appears in a document\n",
    "- TF-IDF reflects term importance (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES ON TF-IDF:\n",
    "- Converting from CountVectorizer to TF-IDF involves transforming raw word counts into a weighted representation that reflects term importance across documents where\n",
    "- CountVectorizer: outputs a term-frequency matrix (e.g. \"The cat sat on the mat\" > {'the':2, 'cat':1, 'sat':1, ...})\n",
    "- TF-IDF adjusts these two counts using Term Frequency (how often a word appears) and Inverse Document Frequency (penalizes terms that may appear in many documents)\n",
    "- Resulting in a boost in rare and meaninful terms and a downweight of common terms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 1: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for 'A Marianas Mosaic: Signs and Shifts in Contemporary Island Life':\n",
      "generation    0.259778\n",
      "other         0.216253\n",
      "people        0.212301\n",
      "culture       0.203745\n",
      "island        0.180117\n",
      "Name: A Marianas Mosaic: Signs and Shifts in Contemporary Island Life, dtype: float64 \n",
      "\n",
      "Top words for 'GUAM: TWO INVASIONS AND THREE MILITARY OCCUPATIONS':\n",
      "japanese     0.430121\n",
      "naval        0.224926\n",
      "gun          0.202889\n",
      "island       0.197352\n",
      "guamanian    0.186958\n",
      "Name: GUAM: TWO INVASIONS AND THREE MILITARY OCCUPATIONS, dtype: float64 \n",
      "\n",
      "Top words for 'Legacy of a Political Union: A Founding Father's Memoir':\n",
      "negotiation    0.353061\n",
      "people         0.331279\n",
      "political      0.221373\n",
      "member         0.191123\n",
      "citizen        0.158975\n",
      "Name: Legacy of a Political Union: A Founding Father's Memoir, dtype: float64 \n",
      "\n",
      "Top words for 'Chamoru Legends: A Gathering of Stories':\n",
      "tree       0.292939\n",
      "brother    0.204505\n",
      "child      0.190687\n",
      "man        0.165218\n",
      "time       0.155640\n",
      "Name: Chamoru Legends: A Gathering of Stories, dtype: float64 \n",
      "\n",
      "Top words for 'Destiny’s Landfall: A History of Guam':\n",
      "ship       0.427217\n",
      "island     0.330497\n",
      "spanish    0.218128\n",
      "man        0.158639\n",
      "galleon    0.146527\n",
      "Name: Destiny’s Landfall: A History of Guam, dtype: float64 \n",
      "\n",
      "Top words for 'The Hale'-Ta Series: HeStorian Taotao Tano': HISTORY OF THE CHAMORRO PEOPLE':\n",
      "people     0.307939\n",
      "ancient    0.286671\n",
      "other      0.272133\n",
      "many       0.200519\n",
      "island     0.190970\n",
      "Name: The Hale'-Ta Series: HeStorian Taotao Tano': HISTORY OF THE CHAMORRO PEOPLE, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer()  # counting how many times each word appears in docs\n",
    "# converts lists of text documents into numerical matrix suitable for machine learning\n",
    "# fit() learns vocabulary from text list\n",
    "# transform converts data into numerical matric based on learned vocabulart\n",
    "# C is a sparse matrix where each row represents a document and columns represent word/term\n",
    "C = vectorizer.fit_transform(text_list) \n",
    "feature_names = vectorizer.get_feature_names_out()  # for mapping column indices to actual words/terms\n",
    "\n",
    "# converting to TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(C)\n",
    "\n",
    "# convert dataframe\n",
    "titles = [metadata['title'] for metadata in metadata_list]\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=feature_names, index=titles)\n",
    "\n",
    "# display top words per document\n",
    "top_n = 5   # adjust for number of words to show\n",
    "for title in titles:\n",
    "    print(f\"Top words for '{title}':\")\n",
    "    top_words = df_tfidf.loc[title].nlargest(top_n)\n",
    "    print(top_words, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 1 (continued): Co-occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjective governing 'guam': past\n",
      "Adjective near 'guam': in\n",
      "Adjective near 'guam': saw\n",
      "Adjective near 'guam': in\n",
      "Adjective near 'guam': neutralize\n",
      "Adjective near 'guam': be\n",
      "Adjective near 'guam': in\n",
      "Adjective near 'guam': confine\n",
      "Adjective near 'guam': in\n",
      "Adjective near 'guam': in\n",
      "Adjective near 'guam': in\n",
      "Adjective near 'guam': from\n",
      "Adjective near 'guam': people\n",
      "Adjective near 'guam': scouted\n",
      "Adjective near 'guam': in\n",
      "Adjective near 'guam': in\n",
      "Adjective near 'guam': settlements\n",
      "Co-occurrences: ['past', 'modern', 'postwar', 'modern', 'strong', 'fortified', 'vulnerable', 'defenseless', 'roadless', 'southern', 'central', 'northern', 'Northern', 'southern', 'northern', 'northern', 'central']\n"
     ]
    }
   ],
   "source": [
    "corpus = raw_text_list\n",
    "co_occurrences = [] # create a list of co-occurrences for target_noun\n",
    "target_noun = 'guam'    # could be anything\n",
    "\n",
    "for doc in corpus:\n",
    "    spacy_doc = nlp(doc)\n",
    "    for token in spacy_doc:\n",
    "        if token.text.lower() == target_noun:\n",
    "            for child in token.children:\n",
    "                if child.pos_ == 'ADJ': # looking for adjecticves near our target noun\n",
    "                    print(f\"Adjective near '{target_noun}': {token.head.text}\")\n",
    "                    co_occurrences.append(child.text)\n",
    "            if token.head.pos_ == 'ADJ':\n",
    "                print(f\"Adjective governing '{target_noun}': {token.head.text}\")    # looking to track heads/chapter names that have co-occurrences\n",
    "                co_occurrences.append(token.head.text)\n",
    "\n",
    "print('Co-occurrences:', co_occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 2: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on Sentiment Analysis:\n",
    "- Sentiment creates a spectrum of sentiment based on negative (-), neutral (0), and positive (+) words throughout the text. In this case, most of the texts are neutral to netural-positive which could tell us something based on the nature / purpose of the texts being written. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  sentiment\n",
      "0  A Marianas Mosaic: Signs and Shifts in Contemp...   0.087177\n",
      "1  GUAM: TWO INVASIONS AND THREE MILITARY OCCUPAT...   0.026302\n",
      "2  Legacy of a Political Union: A Founding Father...   0.112685\n",
      "3            Chamoru Legends: A Gathering of Stories   0.097129\n",
      "4              Destiny’s Landfall: A History of Guam   0.038031\n",
      "5  The Hale'-Ta Series: HeStorian Taotao Tano': H...   0.100835\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# creating a dataframe of metadata to identify parts of the text we're analyzing\n",
    "df_corpus = pd.DataFrame(metadata_list)\n",
    "\n",
    "df_corpus['sentiment'] = [TextBlob(text).sentiment.polarity for text in text_list]\n",
    "print(df_corpus[[\"title\", \"sentiment\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 3: Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES ON TOPIC MODELING:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to INCREASE n_topics (e.g., from 6 to 8 or 10):\n",
    "- If topics seem too broad or contain mixed themes in one topic.\n",
    "- If words from different subjects are appearing together in a single topic.\n",
    "- If you suspect there are more distinct themes in your documents.<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to DECREASE n_topics (e.g., from 6 to 4 or 5):\n",
    "- If topics seem too fragmented, with very specific themes that might not be useful.\n",
    "- If some topics repeat similar themes with slight variations.\n",
    "- If you get many topics that don’t seem meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting text from files. \n",
      "\n",
      "Vectorizing texts... done.\n",
      "\n",
      "Building lda model using training set... done.\n",
      "\n",
      "Top words per topic:\n",
      "Topic 1: guamanians, japan, naval, navy, army, pp, guns, lvt, landing, invasion\n",
      "Topic 2: ancient, vitores, spaniards, maga, missionaries, society, lahi, padre, ancestors, think\n",
      "Topic 3: ko, hilitai, elena, nåna, sirena, carabao, cow, skin, fruit, maybe\n",
      "Topic 4: chamoru, generation, cnmi, halo, filipino, chamorus, healers, refaluwasch, 2017, art\n",
      "\n",
      " model and vectorizer saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "# set pre-work to load XML files\n",
    "folderpath = \"/Users/ricky/digital_texts/corpus/files/0_tei_files/finalized_tei\"\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "# step 1: iterate over all files within folder\n",
    "for file in os.scandir(folderpath):\n",
    "    # control for hidden files\n",
    "    if file.name.startswith('.') or not file.is_file():\n",
    "        continue\n",
    "    try:\n",
    "        with open(file.path, encoding='utf8') as file:\n",
    "            xml_content = file.read()\n",
    "            # parse xml and extract <body> text\n",
    "            soup = bs(xml_content, \"xml\")\n",
    "            text = soup.body.get_text() if soup.body else soup.get_text()\n",
    "            if text.strip():\n",
    "                texts.append(text)\n",
    "                filenames.append(file.name)\n",
    "            else:\n",
    "                print(f'skipping emptry documents: {file.path}')\n",
    "    except UnicodeDecodeError:\n",
    "        print(f'skipping unreadable files: {file.path}')\n",
    "        \n",
    "# step 2: convert to dataframe\n",
    "df = pd.DataFrame({'filename': filenames, 'text': texts})\n",
    "print('Done extracting text from files. \\n')\n",
    "\n",
    "# step 3: vectorize texts\n",
    "print('Vectorizing texts...', end=' ', flush=True)\n",
    "vectorizer = CountVectorizer(min_df=0.01, max_df=0.6, stop_words='english')\n",
    "vectorized_data = vectorizer.fit_transform(df.text)\n",
    "print('done.\\n')\n",
    "\n",
    "# step 4: training lda model\n",
    "print('Building lda model using training set...', end=' ', flush=True)\n",
    "n_topics = 4 # adjust as needed depending on the output\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, learning_decay=0.8, random_state=1)\n",
    "doc_topic_distribution = lda.fit_transform(vectorized_data)\n",
    "print('done.\\n')\n",
    "\n",
    "# step 5: display topic models\n",
    "print('Top words per topic:')\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "    \n",
    "# step 6: assign topics to documents\n",
    "df['topic'] = doc_topic_distribution.argmax(axis=1)\n",
    "\n",
    "# step 7: save model and vectorizer\n",
    "dump(lda, 'lda_model.joblib')\n",
    "dump(vectorizer, 'vectorizer.joblib')\n",
    "\n",
    "print('\\n model and vectorizer saved.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digital_texts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
